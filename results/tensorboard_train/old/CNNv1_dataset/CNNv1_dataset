Connected to pydev debugger (build 173.4127.16)
Tensorflow version 1.8.0
4 files were found under current folder. 
Please be noted that only files end with '*.tfrecord' will be load!
path:/home/radek/DeepLearning/CNN_binary/data/TFRecords/train/train-00000-of-00004.tfrecord-----------------
Found train-00000-of-00004.tfrecord successfully!
path:/home/radek/DeepLearning/CNN_binary/data/TFRecords/train/train-00003-of-00004.tfrecord-----------------
Found train-00003-of-00004.tfrecord successfully!
path:/home/radek/DeepLearning/CNN_binary/data/TFRecords/train/train-00002-of-00004.tfrecord-----------------
Found train-00002-of-00004.tfrecord successfully!
path:/home/radek/DeepLearning/CNN_binary/data/TFRecords/train/train-00001-of-00004.tfrecord-----------------
Found train-00001-of-00004.tfrecord successfully!
/home/radek/DeepLearning/CNN_binary/data/TFRecords/train/train-00000-of-00004.tfrecord
/home/radek/DeepLearning/CNN_binary/data/TFRecords/train/train-00003-of-00004.tfrecord
/home/radek/DeepLearning/CNN_binary/data/TFRecords/train/train-00002-of-00004.tfrecord
/home/radek/DeepLearning/CNN_binary/data/TFRecords/train/train-00001-of-00004.tfrecord
4 files were found under current folder. 
Please be noted that only files end with '*.tfrecord' will be load!
path:/home/radek/DeepLearning/CNN_binary/data/TFRecords/test/test-00000-of-00004.tfrecord-----------------
Found test-00000-of-00004.tfrecord successfully!
path:/home/radek/DeepLearning/CNN_binary/data/TFRecords/test/test-00003-of-00004.tfrecord-----------------
Found test-00003-of-00004.tfrecord successfully!
path:/home/radek/DeepLearning/CNN_binary/data/TFRecords/test/test-00002-of-00004.tfrecord-----------------
Found test-00002-of-00004.tfrecord successfully!
path:/home/radek/DeepLearning/CNN_binary/data/TFRecords/test/test-00001-of-00004.tfrecord-----------------
Found test-00001-of-00004.tfrecord successfully!
/home/radek/DeepLearning/CNN_binary/data/TFRecords/test/test-00000-of-00004.tfrecord
/home/radek/DeepLearning/CNN_binary/data/TFRecords/test/test-00003-of-00004.tfrecord
/home/radek/DeepLearning/CNN_binary/data/TFRecords/test/test-00002-of-00004.tfrecord
/home/radek/DeepLearning/CNN_binary/data/TFRecords/test/test-00001-of-00004.tfrecord
Train dataset size = 96379
Test dataset size = 40142
2018-05-24 14:08:28.778661: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-05-24 14:08:28.838224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-05-24 14:08:28.838600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: GeForce GTX 970M major: 5 minor: 2 memoryClockRate(GHz): 1.038
pciBusID: 0000:01:00.0
totalMemory: 2.94GiB freeMemory: 2.54GiB
2018-05-24 14:08:28.838614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2018-05-24 14:08:29.054334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-05-24 14:08:29.054362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2018-05-24 14:08:29.054368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2018-05-24 14:08:29.054502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2243 MB memory) -> physical GPU (device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0, compute capability: 5.2)
rounded size= 96400
2018-05-24 14:08:32.164523: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.20GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-05-24 14:08:32.250102: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.26GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
0: ********* epoch 1 ********* test accuracy:0.7016341985949878
0: accuracy:0.43 loss: 1114.725 (lr:0.003)
20: accuracy:0.63 loss: 88.741585 (lr:0.0029711445178725875)
40: accuracy:0.63 loss: 89.84227 (lr:0.0029425761525895904)
60: accuracy:0.62 loss: 85.450424 (lr:0.0029142920472906737)
80: accuracy:0.31 loss: 117.548416 (lr:0.0028862893735417373)
100: accuracy:0.53 loss: 101.540565 (lr:0.0028585653310520707)
120: accuracy:0.6 loss: 97.03367 (lr:0.0028311171473943213)
140: accuracy:0.7 loss: 75.102844 (lr:0.0028039420777272502)
160: accuracy:0.66 loss: 69.13944 (lr:0.0027770374045212438)
180: accuracy:0.76 loss: 65.59007 (lr:0.0027504004372865616)
200: accuracy:0.8 loss: 58.273037 (lr:0.0027240285123042826)
220: accuracy:0.74 loss: 64.433075 (lr:0.0026979189923599317)
240: accuracy:0.75 loss: 60.38006 (lr:0.0026720692664797567)
260: accuracy:0.75 loss: 59.758057 (lr:0.0026464767496696276)
280: accuracy:0.72 loss: 69.013306 (lr:0.002621138882656537)
300: accuracy:0.73 loss: 64.515175 (lr:0.0025960531316326675)
320: accuracy:0.76 loss: 56.64747 (lr:0.002571216988002013)
340: accuracy:0.67 loss: 66.345024 (lr:0.002546627968129513)
360: accuracy:0.77 loss: 57.67997 (lr:0.0025222836130926888)
380: accuracy:0.74 loss: 69.691025 (lr:0.0024981814884357505)
400: accuracy:0.83 loss: 47.09063 (lr:0.0024743191839261473)
420: accuracy:0.71 loss: 72.55184 (lr:0.0024506943133135424)
440: accuracy:0.7 loss: 80.697205 (lr:0.0024273045140911875)
460: accuracy:0.74 loss: 61.20398 (lr:0.0024041474472596687)
480: accuracy:0.75 loss: 55.170177 (lr:0.002381220797093005)
500: accuracy:0.69 loss: 64.23882 (lr:0.002358522270907074)
520: accuracy:0.81 loss: 54.342728 (lr:0.0023360495988303423)
540: accuracy:0.75 loss: 59.89403 (lr:0.002313800533576874)
560: accuracy:0.72 loss: 64.91322 (lr:0.0022917728502216037)
580: accuracy:0.82 loss: 45.82885 (lr:0.0022699643459778394)
600: accuracy:0.83 loss: 48.61486 (lr:0.002248372839976982)
620: accuracy:0.78 loss: 64.6509 (lr:0.0022269961730504387)
640: accuracy:0.8 loss: 50.961304 (lr:0.0022058322075137037)
660: accuracy:0.83 loss: 42.841103 (lr:0.0021848788269525857)
680: accuracy:0.83 loss: 48.5808 (lr:0.002164133936011568)
700: accuracy:0.78 loss: 56.77093 (lr:0.002143595460184269)
720: accuracy:0.77 loss: 60.624176 (lr:0.00212326134560599)
740: accuracy:0.81 loss: 41.748646 (lr:0.0021031295588483287)
760: accuracy:0.84 loss: 42.563423 (lr:0.002083198086715832)
780: accuracy:0.85 loss: 38.296707 (lr:0.0020634649360446776)
800: accuracy:0.88 loss: 38.529015 (lr:0.002043928133503354)
820: accuracy:0.85 loss: 32.162167 (lr:0.0020245857253953265)
840: accuracy:0.83 loss: 42.967773 (lr:0.0020054357774636645)
860: accuracy:0.85 loss: 42.195484 (lr:0.001986476374697618)
880: accuracy:0.86 loss: 43.333305 (lr:0.00196770562114111)
900: accuracy:0.83 loss: 54.919643 (lr:0.001949121639703143)
920: accuracy:0.81 loss: 48.303333 (lr:0.0019307225719700854)
940: accuracy:0.87 loss: 38.03618 (lr:0.0019125065780198325)
960: accuracy:0.92 loss: 26.60576 (lr:0.0018944718362378086)
964: ********* epoch 2 ********* test accuracy:0.8414378954710777
980: accuracy:0.88 loss: 36.772106 (lr:0.0018766165431348069)
1000: accuracy:0.9 loss: 26.789478 (lr:0.0018589389131666372)
1020: accuracy:0.9 loss: 34.74971 (lr:0.0018414371785555712)
1040: accuracy:0.88 loss: 35.874184 (lr:0.001824109589113564)
1060: accuracy:0.87 loss: 41.726505 (lr:0.0018069544120672303)
1080: accuracy:0.86 loss: 36.09489 (lr:0.0017899699318845701)
1100: accuracy:0.88 loss: 34.26472 (lr:0.0017731544501034114)
1120: accuracy:0.89 loss: 30.682377 (lr:0.0017565062851615633)
1140: accuracy:0.84 loss: 38.93661 (lr:0.0017400237722286578)
1160: accuracy:0.85 loss: 36.370358 (lr:0.001723705263039666)
1180: accuracy:0.9 loss: 42.64393 (lr:0.0017075491257300707)
1200: accuracy:0.86 loss: 34.76768 (lr:0.0016915537446726768)
1220: accuracy:0.93 loss: 25.46137 (lr:0.0016757175203160495)
1240: accuracy:0.92 loss: 22.763975 (lr:0.001660038869024556)
1260: accuracy:0.89 loss: 36.684917 (lr:0.001644516222920002)
1280: accuracy:0.89 loss: 39.1976 (lr:0.001629148029724841)
1300: accuracy:0.82 loss: 45.37755 (lr:0.0016139327526069466)
1320: accuracy:0.85 loss: 38.505535 (lr:0.0015988688700259279)
1340: accuracy:0.91 loss: 23.95166 (lr:0.0015839548755809732)
1360: accuracy:0.91 loss: 21.980211 (lr:0.0015691892778602098)
1380: accuracy:0.91 loss: 28.859386 (lr:0.0015545706002915614)
1400: accuracy:0.94 loss: 27.121016 (lr:0.0015400973809950877)
1420: accuracy:0.88 loss: 36.0646 (lr:0.001525768172636799)
1440: accuracy:0.92 loss: 28.147238 (lr:0.001511581542283918)
1460: accuracy:0.86 loss: 34.415455 (lr:0.0014975360712615872)
1480: accuracy:0.92 loss: 22.62356 (lr:0.0014836303550109999)
1500: accuracy:0.94 loss: 22.715504 (lr:0.0014698630029489428)
1520: accuracy:0.92 loss: 23.042347 (lr:0.0014562326383287369)
1540: accuracy:0.8 loss: 40.90292 (lr:0.0014427378981025616)
1560: accuracy:0.9 loss: 29.880064 (lr:0.0014293774327851483)
1580: accuracy:0.91 loss: 29.176998 (lr:0.001416149906318832)
1600: accuracy:0.88 loss: 26.328516 (lr:0.0014030539959399427)
1620: accuracy:0.88 loss: 24.887775 (lr:0.0013900883920465294)
1640: accuracy:0.91 loss: 23.686995 (lr:0.001377251798067398)
1660: accuracy:0.93 loss: 18.338545 (lr:0.0013645429303324535)
1680: accuracy:0.97 loss: 16.00228 (lr:0.0013519605179443314)
1700: accuracy:0.93 loss: 34.16594 (lr:0.0013395033026513076)
1720: accuracy:0.92 loss: 19.931572 (lr:0.0013271700387214717)
1740: accuracy:0.89 loss: 31.892256 (lr:0.0013149594928181531)
1760: accuracy:0.87 loss: 38.04723 (lr:0.0013028704438765861)
1780: accuracy:0.89 loss: 31.937126 (lr:0.001290901682981802)
1800: accuracy:0.95 loss: 23.45154 (lr:0.0012790520132477375)
1820: accuracy:0.89 loss: 28.499004 (lr:0.0012673202496975445)
1840: accuracy:0.91 loss: 21.788696 (lr:0.0012557052191450912)
1860: accuracy:0.91 loss: 31.879335 (lr:0.0012442057600776434)
1880: accuracy:0.9 loss: 30.513767 (lr:0.0012328207225397114)
1900: accuracy:0.96 loss: 16.903986 (lr:0.0012215489680180538)
1920: accuracy:0.97 loss: 13.643403 (lr:0.0012103893693278251)
1928: ********* epoch 3 ********* test accuracy:0.8504060584923522
1940: accuracy:0.95 loss: 17.549465 (lr:0.0011993408104998568)
1960: accuracy:0.94 loss: 19.067673 (lr:0.001188402186669059)
1980: accuracy:0.96 loss: 11.20231 (lr:0.0011775724039639326)
2000: accuracy:0.92 loss: 18.651966 (lr:0.0011668503793971828)
2020: accuracy:0.91 loss: 23.634588 (lr:0.0011562350407574179)
2040: accuracy:0.91 loss: 21.87164 (lr:0.0011457253265019273)
2060: accuracy:0.9 loss: 32.573593 (lr:0.0011353201856505275)
2080: accuracy:0.95 loss: 28.221256 (lr:0.0011250185776804627)
2100: accuracy:0.95 loss: 14.701814 (lr:0.0011148194724223506)
2120: accuracy:0.86 loss: 37.603092 (lr:0.0011047218499571666)
2140: accuracy:0.93 loss: 17.685324 (lr:0.0010947247005142493)
2160: accuracy:0.91 loss: 19.95745 (lr:0.0010848270243703235)
2180: accuracy:0.97 loss: 15.036695 (lr:0.0010750278317495268)
2200: accuracy:0.85 loss: 35.166637 (lr:0.0010653261427244307)
2220: accuracy:0.92 loss: 27.158052 (lr:0.0010557209871180483)
2240: accuracy:0.94 loss: 20.296743 (lr:0.0010462114044068145)
2260: accuracy:0.94 loss: 25.697227 (lr:0.0010367964436245336)
2280: accuracy:0.91 loss: 22.792091 (lr:0.0010274751632672816)
2300: accuracy:0.95 loss: 16.466496 (lr:0.0010182466311992545)
2320: accuracy:0.92 loss: 25.037844 (lr:0.0010091099245595554)
2340: accuracy:0.94 loss: 19.24317 (lr:0.0010000641296699067)
2360: accuracy:0.93 loss: 21.221346 (lr:0.0009911083419432806)
2380: accuracy:0.9 loss: 36.65831 (lr:0.0009822416657934419)
2400: accuracy:0.94 loss: 14.193049 (lr:0.0009734632145453863)
2420: accuracy:0.93 loss: 26.584759 (lr:0.0009647721103466736)
2440: accuracy:0.92 loss: 22.693295 (lr:0.0009561674840796413)
2460: accuracy:0.92 loss: 25.94288 (lr:0.0009476484752744924)
2480: accuracy:0.93 loss: 16.487246 (lr:0.0009392142320232469)
2500: accuracy:0.95 loss: 15.438777 (lr:0.0009308639108945514)
2520: accuracy:0.84 loss: 35.932808 (lr:0.0009225966768493343)
2540: accuracy:0.92 loss: 20.763382 (lr:0.0009144117031573014)
2560: accuracy:0.92 loss: 19.598114 (lr:0.0009063081713142631)
2580: accuracy:0.93 loss: 18.660357 (lr:0.0008982852709602818)
2600: accuracy:0.92 loss: 27.626347 (lr:0.0008903421997986366)
2620: accuracy:0.97 loss: 9.204026 (lr:0.0008824781635155918)
2640: accuracy:0.97 loss: 14.783071 (lr:0.000874692375700966)
2660: accuracy:0.95 loss: 15.1859045 (lr:0.0008669840577694896)
2680: accuracy:0.94 loss: 21.80891 (lr:0.0008593524388829455)
2700: accuracy:0.99 loss: 10.651297 (lr:0.0008517967558730855)
2720: accuracy:0.89 loss: 23.415493 (lr:0.0008443162531653122)
2740: accuracy:0.96 loss: 14.910375 (lr:0.0008369101827031209)
2760: accuracy:0.92 loss: 21.53096 (lr:0.000829577803873294)
2780: accuracy:0.96 loss: 20.0435 (lr:0.000822318383431838)
2800: accuracy:0.94 loss: 21.294268 (lr:0.0008151311954306589)
2820: accuracy:0.94 loss: 18.423286 (lr:0.0008080155211449677)
2840: accuracy:0.89 loss: 20.913593 (lr:0.0008009706490014058)
2860: accuracy:0.96 loss: 15.106056 (lr:0.0007939958745068883)
2880: accuracy:0.99 loss: 6.3230114 (lr:0.0007870905001781532)
2892: ********* epoch 4 ********* test accuracy:0.8317971202232076
2900: accuracy:0.97 loss: 6.964315 (lr:0.0007802538354720133)
2920: accuracy:0.98 loss: 6.455 (lr:0.0007734851967163007)
2940: accuracy:0.97 loss: 10.014653 (lr:0.0007667839070414992)
2960: accuracy:0.96 loss: 14.603582 (lr:0.000760149296313057)
2980: accuracy:0.91 loss: 25.333292 (lr:0.0007535807010643724)
3000: accuracy:0.95 loss: 15.412645 (lr:0.0007470774644304465)
3020: accuracy:0.96 loss: 15.312776 (lr:0.0007406389360821968)
3040: accuracy:0.98 loss: 6.2826114 (lr:0.0007342644721614229)
3060: accuracy:0.96 loss: 18.018755 (lr:0.0007279534352164206)
3080: accuracy:0.94 loss: 11.071683 (lr:0.0007217051941382361)
3100: accuracy:0.92 loss: 22.67331 (lr:0.0007155191240975549)
3120: accuracy:0.97 loss: 8.110155 (lr:0.0007093946064822178)
3140: accuracy:0.96 loss: 13.59026 (lr:0.0007033310288353594)
3160: accuracy:0.95 loss: 15.156087 (lr:0.000697327784794162)
3180: accuracy:0.96 loss: 18.849997 (lr:0.000691384274029219)
3200: accuracy:0.97 loss: 12.75312 (lr:0.0006854999021845007)
3220: accuracy:0.91 loss: 17.546114 (lr:0.0006796740808179191)
3240: accuracy:0.97 loss: 13.65531 (lr:0.0006739062273424826)
3260: accuracy:0.96 loss: 14.008914 (lr:0.0006681957649680373)
3280: accuracy:0.98 loss: 8.72946 (lr:0.0006625421226435866)
3300: accuracy:0.96 loss: 11.218596 (lr:0.000656944735000187)
3320: accuracy:0.95 loss: 17.387962 (lr:0.0006514030422944097)
3340: accuracy:0.97 loss: 12.288626 (lr:0.0006459164903523658)
3360: accuracy:0.95 loss: 16.904913 (lr:0.000640484530514289)
3380: accuracy:0.92 loss: 30.431826 (lr:0.000635106619579669)
3400: accuracy:0.96 loss: 17.103546 (lr:0.0006297822197529307)
3420: accuracy:0.96 loss: 9.620508 (lr:0.0006245107985896541)
3440: accuracy:0.96 loss: 12.39632 (lr:0.0006192918289433304)
3460: accuracy:0.97 loss: 9.865921 (lr:0.0006141247889126458)
3480: accuracy:0.95 loss: 10.442979 (lr:0.000609009161789291)
3500: accuracy:0.97 loss: 10.281707 (lr:0.000603944436006291)
3520: accuracy:0.96 loss: 14.087436 (lr:0.0005989301050868467)
3540: accuracy:0.92 loss: 15.9953785 (lr:0.0005939656675936874)
3560: accuracy:0.97 loss: 6.8291163 (lr:0.000589050627078927)
3580: accuracy:0.96 loss: 12.32525 (lr:0.000584184492034418)
3600: accuracy:0.97 loss: 9.832567 (lr:0.000579366775842601)
3620: accuracy:0.97 loss: 8.729992 (lr:0.0005745969967278418)
3640: accuracy:0.96 loss: 11.608784 (lr:0.0005698746777082543)
3660: accuracy:0.98 loss: 10.378903 (lr:0.000565199346548001)
3680: accuracy:0.97 loss: 7.469936 (lr:0.00056057053571007)
3700: accuracy:0.97 loss: 8.080959 (lr:0.0005559877823095201)
3720: accuracy:0.98 loss: 10.2165985 (lr:0.0005514506280671922)
3740: accuracy:0.98 loss: 6.4945393 (lr:0.0005469586192638811)
3760: accuracy:0.98 loss: 9.865491 (lr:0.0005425113066949633)
3780: accuracy:0.92 loss: 14.524315 (lr:0.0005381082456254755)
3800: accuracy:0.97 loss: 16.663225 (lr:0.0005337489957456418)
3820: accuracy:0.97 loss: 6.7868934 (lr:0.0005294331211268412)
3840: accuracy:0.99 loss: 4.7012043 (lr:0.0005251601901780155)
3856: ********* epoch 5 ********* test accuracy:0.835982262966469
3860: accuracy:1.0 loss: 2.0544808 (lr:0.0005209297756025089)
3880: accuracy:1.0 loss: 2.3565402 (lr:0.0005167414543553385)
3900: accuracy:0.99 loss: 7.245276 (lr:0.0005125948076008895)
3920: accuracy:0.95 loss: 11.941962 (lr:0.0005084894206710306)
3940: accuracy:0.96 loss: 10.242525 (lr:0.0005044248830236478)
3960: accuracy:0.97 loss: 8.095859 (lr:0.0005004007882015892)
3980: accuracy:0.97 loss: 11.012509 (lr:0.0004964167337920192)
4000: accuracy:0.99 loss: 6.862733 (lr:0.0004924723213861769)
4020: accuracy:1.0 loss: 3.7697115 (lr:0.0004885671565395345)
4040: accuracy:0.97 loss: 7.7542677 (lr:0.00048470084873235297)
4060: accuracy:1.0 loss: 4.327408 (lr:0.00048087301133063005)
4080: accuracy:0.97 loss: 12.275865 (lr:0.00047708326154743513)
4100: accuracy:1.0 loss: 3.7120552 (lr:0.0004733312204046323)
4120: accuracy:0.95 loss: 11.877567 (lr:0.0004696165126949802)
4140: accuracy:0.96 loss: 15.017958 (lr:0.00046593876694461247)
4160: accuracy:0.96 loss: 9.965959 (lr:0.000462297615375889)
4180: accuracy:0.93 loss: 26.020632 (lr:0.000458692693870619)
4200: accuracy:0.99 loss: 5.323557 (lr:0.00045512364193364755)
4220: accuracy:0.96 loss: 7.6016946 (lr:0.0004515901026568069)
4240: accuracy:0.98 loss: 7.2144074 (lr:0.00044809172268322453)
4260: accuracy:0.99 loss: 8.031658 (lr:0.00044462815217198804)
4280: accuracy:0.98 loss: 14.173523 (lr:0.0004411990447631597)
4300: accuracy:1.0 loss: 4.335878 (lr:0.00043780405754314123)
4320: accuracy:0.99 loss: 3.9208274 (lr:0.0004344428510103813)
4340: accuracy:0.99 loss: 5.065675 (lr:0.00043111508904142585)
4360: accuracy:0.98 loss: 5.584427 (lr:0.0004278204388573046)
4380: accuracy:1.0 loss: 4.7147655 (lr:0.0004245585709902538)
4400: accuracy:0.99 loss: 8.985721 (lr:0.00042132915925076824)
4420: accuracy:1.0 loss: 3.084465 (lr:0.0004181318806949831)
4440: accuracy:0.99 loss: 4.1963253 (lr:0.0004149664155923781)
4460: accuracy:0.98 loss: 5.4805813 (lr:0.0004118324473938054)
4480: accuracy:0.99 loss: 4.793085 (lr:0.00040872966269983314)
4500: accuracy:0.98 loss: 8.21338 (lr:0.00040565775122940656)
4520: accuracy:0.97 loss: 8.398892 (lr:0.0004026164057888186)
4540: accuracy:1.0 loss: 2.1555197 (lr:0.0003996053222409906)
4560: accuracy:0.98 loss: 9.028538 (lr:0.0003966241994750587)
4580: accuracy:0.97 loss: 10.219898 (lr:0.00039367273937626184)
4600: accuracy:0.99 loss: 5.8185234 (lr:0.0003907506467961309)
4620: accuracy:0.99 loss: 6.477914 (lr:0.0003878576295229724)
4640: accuracy:0.97 loss: 10.873919 (lr:0.0003849933982526485)
4660: accuracy:0.99 loss: 6.356196 (lr:0.00038215766655964504)
4680: accuracy:0.97 loss: 9.578356 (lr:0.0003793501508684298)
4700: accuracy:1.0 loss: 3.1736004 (lr:0.0003765705704250939)
4720: accuracy:0.96 loss: 11.53312 (lr:0.0003738186472692768)
4740: accuracy:0.99 loss: 3.361268 (lr:0.0003710941062063696)
4760: accuracy:0.99 loss: 7.1456304 (lr:0.00036839667477999555)
4780: accuracy:1.0 loss: 1.1446635 (lr:0.00036572608324476403)
4800: accuracy:0.99 loss: 2.4285161 (lr:0.0003630820645392963)
4820: ********* epoch 6 ********* test accuracy:0.826266753026755
4820: accuracy:1.0 loss: 0.49339104 (lr:0.00036046435425951816)
4840: accuracy:1.0 loss: 0.7533966 (lr:0.00035787269063222043)
4860: accuracy:0.99 loss: 2.1122248 (lr:0.0003553068144888804)
4880: accuracy:1.0 loss: 1.3479033 (lr:0.00035276646923974576)
4900: accuracy:1.0 loss: 0.74972504 (lr:0.00035025140084817447)
4920: accuracy:0.98 loss: 5.368285 (lr:0.0003477613578052316)
4940: accuracy:0.98 loss: 8.027875 (lr:0.00034529609110453763)
4960: accuracy:0.99 loss: 5.8987823 (lr:0.0003428553542173683)
4980: accuracy:0.99 loss: 2.6312282 (lr:0.00034043890306800073)
5000: accuracy:1.0 loss: 2.2332835 (lr:0.00033804649600930654)
5020: accuracy:1.0 loss: 2.4901814 (lr:0.00033567789379858597)
5040: accuracy:0.99 loss: 4.04311 (lr:0.0003333328595736441)
5060: accuracy:1.0 loss: 2.2331152 (lr:0.00033101115882910436)
5080: accuracy:1.0 loss: 3.7591624 (lr:0.00032871255939295735)
5100: accuracy:1.0 loss: 4.0926585 (lr:0.0003264368314033442)
5120: accuracy:0.97 loss: 6.048178 (lr:0.0003241837472855693)
5140: accuracy:0.98 loss: 8.729265 (lr:0.00032195308172934344)
5160: accuracy:0.99 loss: 5.8880525 (lr:0.00031974461166625194)
5180: accuracy:1.0 loss: 3.121286 (lr:0.00031755811624744826)
5200: accuracy:0.98 loss: 7.5017953 (lr:0.0003153933768215683)
5220: accuracy:1.0 loss: 2.6422408 (lr:0.0003132501769128656)
5240: accuracy:0.99 loss: 6.2130704 (lr:0.0003111283021995632)
5260: accuracy:0.99 loss: 8.264964 (lr:0.00030902754049242173)
5280: accuracy:0.98 loss: 5.0201454 (lr:0.0003069476817135196)
5300: accuracy:0.98 loss: 2.3562744 (lr:0.00030488851787524585)
5320: accuracy:0.98 loss: 4.9220076 (lr:0.0003028498430595006)
5340: accuracy:1.0 loss: 2.918495 (lr:0.0003008314533971034)
5360: accuracy:1.0 loss: 3.3607955 (lr:0.00029883314704740597)
5380: accuracy:0.98 loss: 5.623838 (lr:0.0002968547241781082)
5400: accuracy:0.97 loss: 10.886918 (lr:0.0002948959869452743)
5420: accuracy:1.0 loss: 2.2144992 (lr:0.0002929567394735489)
5440: accuracy:0.97 loss: 6.528695 (lr:0.00029103678783656855)
5460: accuracy:0.99 loss: 5.456801 (lr:0.00028913594003756986)
5480: accuracy:1.0 loss: 2.6156902 (lr:0.00028725400599018857)
5500: accuracy:0.97 loss: 5.1871495 (lr:0.00028539079749945195)
5520: accuracy:0.98 loss: 5.9304724 (lr:0.00028354612824295816)
5540: accuracy:0.98 loss: 10.541394 (lr:0.0002817198137522442)
5560: accuracy:0.97 loss: 6.164655 (lr:0.00027991167139433914)
5580: accuracy:1.0 loss: 1.6710125 (lr:0.0002781215203535004)
5600: accuracy:1.0 loss: 3.5841444 (lr:0.00027634918161313215)
5620: accuracy:0.98 loss: 4.702301 (lr:0.0002745944779378833)
5640: accuracy:1.0 loss: 2.0995958 (lr:0.0002728572338559242)
5660: accuracy:1.0 loss: 1.4764922 (lr:0.0002711372756413988)
5680: accuracy:0.99 loss: 6.8001823 (lr:0.0002694344312970524)
5700: accuracy:0.99 loss: 8.2141905 (lr:0.0002677485305370315)
5720: accuracy:0.97 loss: 6.2796397 (lr:0.00026607940476985534)
5740: accuracy:1.0 loss: 0.69821006 (lr:0.00026442688708155606)
5760: accuracy:1.0 loss: 0.55543685 (lr:0.0002627908122189878)
5780: accuracy:1.0 loss: 0.76229924 (lr:0.0002611710165733009)
5784: ********* epoch 7 ********* test accuracy:0.8254446714164715
5800: accuracy:1.0 loss: 0.6861141 (lr:0.000259567338163581)
5820: accuracy:1.0 loss: 0.84434336 (lr:0.0002579796166206506)
5840: accuracy:1.0 loss: 1.694016 (lr:0.00025640769317103245)
5860: accuracy:0.99 loss: 2.9691749 (lr:0.00025485141062107154)
5880: accuracy:0.98 loss: 18.490751 (lr:0.00025331061334121606)
5900: accuracy:0.99 loss: 4.053729 (lr:0.00025178514725045394)
5920: accuracy:0.99 loss: 3.3061988 (lr:0.00025027485980090493)
5940: accuracy:0.99 loss: 3.0524402 (lr:0.00024877959996256544)
5960: accuracy:1.0 loss: 2.2707422 (lr:0.00024729921820820566)
5980: accuracy:0.99 loss: 4.073869 (lr:0.0002458335664984164)
6000: accuracy:0.99 loss: 4.1954274 (lr:0.00024438249826680544)
6020: accuracy:0.99 loss: 3.6841922 (lr:0.0002429458684053403)
6040: accuracy:1.0 loss: 1.787394 (lr:0.00024152353324983762)
6060: accuracy:0.99 loss: 2.6933687 (lr:0.00024011535056559663)
6080: accuracy:0.98 loss: 6.9734974 (lr:0.00023872117953317528)
6100: accuracy:0.99 loss: 3.1367435 (lr:0.00023734088073430873)
6120: accuracy:0.98 loss: 5.905153 (lr:0.00023597431613796662)
6140: accuracy:0.99 loss: 4.598214 (lr:0.0002346213490865507)
6160: accuracy:1.0 loss: 2.2142737 (lr:0.00023328184428222823)
6180: accuracy:0.99 loss: 1.8901256 (lr:0.00023195566777340254)
6200: accuracy:1.0 loss: 1.7143812 (lr:0.00023064268694131766)
6220: accuracy:0.99 loss: 3.0985425 (lr:0.00022934277048679616)
6240: accuracy:0.99 loss: 8.706476 (lr:0.00022805578841710933)
6260: accuracy:0.98 loss: 9.62045 (lr:0.00022678161203297775)
6280: accuracy:1.0 loss: 1.7948579 (lr:0.0002255201139157011)
6300: accuracy:0.98 loss: 3.1400404 (lr:0.00022427116791441654)
6320: accuracy:0.99 loss: 2.36419 (lr:0.00022303464913348302)
6340: accuracy:0.99 loss: 2.5178394 (lr:0.0002218104339199921)
6360: accuracy:0.99 loss: 2.729783 (lr:0.00022059839985140217)
6380: accuracy:0.97 loss: 6.8716736 (lr:0.00021939842572329645)
6400: accuracy:0.99 loss: 6.4449744 (lr:0.00021821039153726202)
6420: accuracy:1.0 loss: 2.2257714 (lr:0.00021703417848889034)
6440: accuracy:0.99 loss: 2.5336409 (lr:0.0002158696689558963)
6460: accuracy:0.98 loss: 6.068747 (lr:0.0002147167464863563)
6480: accuracy:1.0 loss: 1.036868 (lr:0.00021357529578706251)
6500: accuracy:1.0 loss: 0.84478843 (lr:0.00021244520271199385)
6520: accuracy:0.99 loss: 2.1396434 (lr:0.00021132635425090104)
6540: accuracy:1.0 loss: 1.7384841 (lr:0.00021021863851800553)
6560: accuracy:0.99 loss: 5.5404525 (lr:0.00020912194474081107)
6580: accuracy:0.99 loss: 3.3979583 (lr:0.00020803616324902585)
6600: accuracy:0.99 loss: 3.0642989 (lr:0.00020696118546359606)
6620: accuracy:1.0 loss: 1.9520203 (lr:0.00020589690388584717)
6640: accuracy:1.0 loss: 0.9301891 (lr:0.00020484321208673464)
6660: accuracy:1.0 loss: 1.3387023 (lr:0.00020380000469620037)
6680: accuracy:0.96 loss: 11.873703 (lr:0.00020276717739263608)
6700: accuracy:0.99 loss: 4.006809 (lr:0.0002017446268924506)
6720: accuracy:1.0 loss: 0.47290394 (lr:0.00020073225093974185)
6740: accuracy:1.0 loss: 0.6004795 (lr:0.00019972994829607087)
6748: ********* epoch 8 ********* test accuracy:0.8273379502765184
6760: accuracy:1.0 loss: 0.83750963 (lr:0.00019873761873033811)
6780: accuracy:1.0 loss: 0.5424791 (lr:0.00019775516300875995)
6800: accuracy:1.0 loss: 0.9111278 (lr:0.00019678248288494563)
6820: accuracy:1.0 loss: 0.5782522 (lr:0.00019581948109007212)
6840: accuracy:0.99 loss: 3.0131156 (lr:0.0001948660613231575)
6860: accuracy:1.0 loss: 0.9644071 (lr:0.00019392212824143043)
6880: accuracy:0.97 loss: 11.0713005 (lr:0.00019298758745079625)
6900: accuracy:0.98 loss: 4.918394 (lr:0.00019206234549639704)
6920: accuracy:0.99 loss: 2.0977192 (lr:0.00019114630985326637)
6940: accuracy:0.98 loss: 6.8165636 (lr:0.0001902393889170765)
6960: accuracy:1.0 loss: 0.8638249 (lr:0.0001893414919949781)
6980: accuracy:0.99 loss: 2.7183356 (lr:0.0001884525292965307)
7000: accuracy:1.0 loss: 0.8003517 (lr:0.00018757241192472366)
7020: accuracy:0.98 loss: 4.156447 (lr:0.00018670105186708636)
7040: accuracy:0.97 loss: 7.641616 (lr:0.00018583836198688683)
7060: accuracy:1.0 loss: 1.0575738 (lr:0.00018498425601441823)
7080: accuracy:0.99 loss: 5.4536304 (lr:0.00018413864853837148)
7100: accuracy:1.0 loss: 1.8675649 (lr:0.00018330145499729437)
7120: accuracy:0.99 loss: 2.7633362 (lr:0.00018247259167113508)
7140: accuracy:1.0 loss: 2.5861104 (lr:0.0001816519756728703)
7160: accuracy:1.0 loss: 1.8294983 (lr:0.00018083952494021637)
7180: accuracy:1.0 loss: 2.4500687 (lr:0.00018003515822742294)
7200: accuracy:0.99 loss: 6.8755865 (lr:0.00017923879509714843)
7220: accuracy:1.0 loss: 1.0030285 (lr:0.0001784503559124162)
7240: accuracy:1.0 loss: 2.720305 (lr:0.0001776697618286507)
7260: accuracy:0.99 loss: 2.5380034 (lr:0.00017689693478579313)
7280: accuracy:0.99 loss: 3.700777 (lr:0.0001761317975004951)
7300: accuracy:1.0 loss: 1.0398333 (lr:0.0001753742734583905)
7320: accuracy:1.0 loss: 0.76899725 (lr:0.00017462428690644382)
7340: accuracy:0.99 loss: 3.1771085 (lr:0.00017388176284537495)
7360: accuracy:1.0 loss: 2.697997 (lr:0.0001731466270221589)
7380: accuracy:0.98 loss: 3.7606456 (lr:0.00017241880592260087)
7400: accuracy:0.99 loss: 3.0727227 (lr:0.00017169822676398424)
7420: accuracy:0.99 loss: 4.228164 (lr:0.00017098481748779274)
7440: accuracy:1.0 loss: 0.66575193 (lr:0.00017027850675250424)
7460: accuracy:0.99 loss: 1.9054884 (lr:0.0001695792239264566)
7480: accuracy:0.99 loss: 3.7635717 (lr:0.0001688868990807845)
7500: accuracy:0.98 loss: 2.724311 (lr:0.00016820146298242642)
7520: accuracy:1.0 loss: 0.74637973 (lr:0.00016752284708720135)
7540: accuracy:1.0 loss: 2.4648724 (lr:0.00016685098353295415)
7560: accuracy:0.99 loss: 2.4071867 (lr:0.00016618580513276965)
7580: accuracy:1.0 loss: 1.9107705 (lr:0.00016552724536825341)
7600: accuracy:0.98 loss: 4.1057467 (lr:0.00016487523838288026)
7620: accuracy:1.0 loss: 1.5802768 (lr:0.00016422971897540824)
7640: accuracy:1.0 loss: 1.3886418 (lr:0.00016359062259335873)
7660: accuracy:0.99 loss: 3.9348981 (lr:0.00016295788532656085)
7680: accuracy:1.0 loss: 0.32976815 (lr:0.00016233144390076078)
7700: accuracy:1.0 loss: 0.33729172 (lr:0.0001617112356712938)
7712: ********* epoch 9 ********* test accuracy:0.8319465896068955
7720: accuracy:1.0 loss: 0.15747014 (lr:0.00016109719861682017)
7740: accuracy:1.0 loss: 0.32132939 (lr:0.00016048927133312268)
7760: accuracy:0.99 loss: 1.275477 (lr:0.00015988739302696644)
7780: accuracy:1.0 loss: 1.143283 (lr:0.0001592915035100192)
7800: accuracy:1.0 loss: 1.8012352 (lr:0.00015870154319283275)
7820: accuracy:0.99 loss: 4.359449 (lr:0.00015811745307888365)
7840: accuracy:0.99 loss: 3.2106068 (lr:0.00015753917475867384)
7860: accuracy:1.0 loss: 1.1943595 (lr:0.00015696665040388936)
7880: accuracy:1.0 loss: 1.3563764 (lr:0.00015639982276161764)
7900: accuracy:1.0 loss: 1.0962505 (lr:0.00015583863514862209)
7920: accuracy:0.99 loss: 4.989685 (lr:0.00015528303144567374)
7940: accuracy:1.0 loss: 0.314242 (lr:0.00015473295609193932)
7960: accuracy:0.99 loss: 3.55238 (lr:0.00015418835407942505)
7980: accuracy:1.0 loss: 2.0336354 (lr:0.0001536491709474758)
8000: accuracy:1.0 loss: 1.0189891 (lr:0.00015311535277732913)
8020: accuracy:1.0 loss: 1.4883004 (lr:0.00015258684618672314)
8040: accuracy:0.98 loss: 3.3335254 (lr:0.00015206359832455833)
8060: accuracy:1.0 loss: 1.292695 (lr:0.00015154555686561238)
8080: accuracy:1.0 loss: 1.0317707 (lr:0.00015103267000530785)
8100: accuracy:1.0 loss: 0.89413387 (lr:0.0001505248864545312)
8120: accuracy:1.0 loss: 0.7887622 (lr:0.00015002215543450422)
8140: accuracy:0.99 loss: 3.906081 (lr:0.00014952442667170591)
8160: accuracy:1.0 loss: 0.79308933 (lr:0.0001490316503928453)
8180: accuracy:1.0 loss: 2.3556006 (lr:0.00014854377731988384)
8200: accuracy:0.99 loss: 4.0382543 (lr:0.00014806075866510764)
8220: accuracy:1.0 loss: 1.019997 (lr:0.00014758254612624869)
8240: accuracy:1.0 loss: 1.10661 (lr:0.00014710909188165463)
8260: accuracy:1.0 loss: 1.0904856 (lr:0.00014664034858550645)
8280: accuracy:1.0 loss: 1.3734925 (lr:0.00014617626936308393)
8300: accuracy:1.0 loss: 0.8541149 (lr:0.00014571680780607802)
8320: accuracy:1.0 loss: 1.5825039 (lr:0.00014526191796795023)
8340: accuracy:1.0 loss: 1.583404 (lr:0.0001448115543593376)
8360: accuracy:1.0 loss: 1.5558186 (lr:0.00014436567194350402)
8380: accuracy:1.0 loss: 0.6758003 (lr:0.00014392422613183626)
8400: accuracy:1.0 loss: 0.8166681 (lr:0.00014348717277938534)
8420: accuracy:0.99 loss: 3.077169 (lr:0.0001430544681804518)
8440: accuracy:1.0 loss: 0.7634755 (lr:0.00014262606906421518)
8460: accuracy:1.0 loss: 1.55145 (lr:0.00014220193259040676)
8480: accuracy:1.0 loss: 1.3276064 (lr:0.00014178201634502583)
8500: accuracy:1.0 loss: 0.64410067 (lr:0.00014136627833609785)
8520: accuracy:0.99 loss: 1.9428596 (lr:0.00014095467698947548)
8540: accuracy:1.0 loss: 0.47789818 (lr:0.0001405471711446811)
8560: accuracy:1.0 loss: 0.49097753 (lr:0.00014014372005079055)
8580: accuracy:1.0 loss: 0.96522707 (lr:0.00013974428336235834)
8600: accuracy:1.0 loss: 1.4942383 (lr:0.00013934882113538272)
8620: accuracy:0.98 loss: 9.733309 (lr:0.00013895729382331143)
8640: accuracy:0.99 loss: 1.0790333 (lr:0.00013856966227308694)
8660: accuracy:1.0 loss: 0.18956071 (lr:0.0001381858877212313)
8676: ********* epoch 10 ********* test accuracy:0.8224054606148175
8680: accuracy:1.0 loss: 0.22904067 (lr:0.00013780593178996944)
8700: accuracy:1.0 loss: 0.32713598 (lr:0.00013742975648339164)
8720: accuracy:0.99 loss: 3.6476207 (lr:0.00013705732418365372)
8740: accuracy:1.0 loss: 1.091226 (lr:0.0001366885976472154)
8760: accuracy:1.0 loss: 0.4365065 (lr:0.00013632354000111572)
8780: accuracy:1.0 loss: 0.8375588 (lr:0.00013596211473928589)
8800: accuracy:1.0 loss: 1.0100307 (lr:0.00013560428571889846)
8820: accuracy:0.99 loss: 1.4714732 (lr:0.00013525001715675332)
8840: accuracy:1.0 loss: 0.45417786 (lr:0.00013489927362569896)
8860: accuracy:1.0 loss: 0.6015303 (lr:0.00013455202005109)
8880: accuracy:1.0 loss: 0.44467565 (lr:0.00013420822170727954)
8900: accuracy:1.0 loss: 1.1090895 (lr:0.0001338678442141468)
8920: accuracy:1.0 loss: 0.66088706 (lr:0.00013353085353365877)
8940: accuracy:1.0 loss: 0.7912271 (lr:0.00013319721596646657)
8960: accuracy:0.99 loss: 1.4809519 (lr:0.00013286689814853543)
8980: accuracy:1.0 loss: 1.8376479 (lr:0.00013253986704780835)
9000: accuracy:0.97 loss: 9.30462 (lr:0.0001322160899609027)
9020: accuracy:1.0 loss: 0.6966919 (lr:0.00013189553450983996)
9040: accuracy:1.0 loss: 0.61551714 (lr:0.00013157816863880792)
9060: accuracy:1.0 loss: 0.5510384 (lr:0.00013126396061095494)
9080: accuracy:1.0 loss: 1.1287752 (lr:0.00013095287900521648)
9100: accuracy:0.98 loss: 6.4983354 (lr:0.00013064489271317272)
9120: accuracy:1.0 loss: 0.9657075 (lr:0.00013033997093593773)
9140: accuracy:1.0 loss: 1.2431098 (lr:0.00013003808318107972)
9160: accuracy:1.0 loss: 1.1443526 (lr:0.00012973919925957168)
9180: accuracy:1.0 loss: 0.7114156 (lr:0.00012944328928277232)
9200: accuracy:1.0 loss: 0.70265555 (lr:0.0001291503236594374)
9220: accuracy:0.99 loss: 4.1291456 (lr:0.00012886027309276044)
9240: accuracy:1.0 loss: 0.40169382 (lr:0.00012857310857744306)
9260: accuracy:1.0 loss: 0.55858463 (lr:0.00012828880139679441)
9280: accuracy:1.0 loss: 0.44034588 (lr:0.00012800732311985956)
9300: accuracy:1.0 loss: 0.8951272 (lr:0.00012772864559857617)
9320: accuracy:0.99 loss: 2.4842193 (lr:0.00012745274096495995)
9340: accuracy:1.0 loss: 1.1246164 (lr:0.00012717958162831758)
9360: accuracy:1.0 loss: 0.38579005 (lr:0.00012690914027248777)
9380: accuracy:0.99 loss: 2.8700438 (lr:0.00012664138985310952)
9400: accuracy:0.99 loss: 2.2997632 (lr:0.00012637630359491788)
9420: accuracy:1.0 loss: 0.70259666 (lr:0.00012611385498906603)
9440: accuracy:1.0 loss: 0.8289745 (lr:0.00012585401779047472)
9460: accuracy:0.98 loss: 3.6163216 (lr:0.0001255967660152075)
9480: accuracy:0.99 loss: 2.7138171 (lr:0.00012534207393787256)
9500: accuracy:1.0 loss: 0.7180451 (lr:0.00012508991608904985)
9520: accuracy:1.0 loss: 0.4010797 (lr:0.00012484026725274438)
9540: accuracy:0.99 loss: 2.024564 (lr:0.00012459310246386448)
9560: accuracy:1.0 loss: 1.0022278 (lr:0.00012434839700572526)
9580: accuracy:1.0 loss: 0.44270512 (lr:0.00012410612640757705)
9600: accuracy:1.0 loss: 0.23212633 (lr:0.0001238662664421581)
9620: accuracy:1.0 loss: 0.19181517 (lr:0.00012362879312327198)
9640: ********* epoch 11 ********* test accuracy:0.8250959095211997
9640: accuracy:1.0 loss: 0.051098865 (lr:0.0001233936827033889)
9660: accuracy:1.0 loss: 0.05360221 (lr:0.00012316091167127095)
9680: accuracy:1.0 loss: 0.6172248 (lr:0.000122930456749621)
9700: accuracy:1.0 loss: 0.5648459 (lr:0.00012270229489275475)
9720: accuracy:1.0 loss: 0.17951237 (lr:0.00012247640328429642)
9740: accuracy:1.0 loss: 0.23036996 (lr:0.00012225275933489693)
9760: accuracy:1.0 loss: 0.9103538 (lr:0.00012203134067997495)
9780: accuracy:0.99 loss: 1.502468 (lr:0.00012181212517748049)
9800: accuracy:1.0 loss: 0.3969815 (lr:0.00012159509090568058)
9820: accuracy:1.0 loss: 0.309029 (lr:0.00012138021616096724)
9840: accuracy:1.0 loss: 0.38399988 (lr:0.00012116747945568689)
9860: accuracy:0.99 loss: 1.7811408 (lr:0.00012095685951599174)
9880: accuracy:1.0 loss: 0.1965326 (lr:0.00012074833527971228)
9900: accuracy:1.0 loss: 0.5061447 (lr:0.00012054188589425116)
9920: accuracy:1.0 loss: 0.4393574 (lr:0.00012033749071449773)
9940: accuracy:1.0 loss: 0.67675704 (lr:0.00012013512930076374)
9960: accuracy:0.99 loss: 2.293119 (lr:0.00011993478141673913)
9980: accuracy:1.0 loss: 0.5398379 (lr:0.00011973642702746858)
10000: accuracy:1.0 loss: 0.5071459 (lr:0.00011954004629734786)
10020: accuracy:1.0 loss: 1.5625991 (lr:0.0001193456195881403)
10040: accuracy:1.0 loss: 1.0978924 (lr:0.00011915312745701295)
10060: accuracy:1.0 loss: 0.7923807 (lr:0.0001189625506545923)
10080: accuracy:1.0 loss: 0.90587354 (lr:0.00011877387012303928)
10100: accuracy:1.0 loss: 0.86124253 (lr:0.00011858706699414352)
10120: accuracy:1.0 loss: 0.109009326 (lr:0.00011840212258743644)
10140: accuracy:1.0 loss: 0.5715197 (lr:0.00011821901840832325)
10160: accuracy:1.0 loss: 0.31677377 (lr:0.00011803773614623347)
10180: accuracy:1.0 loss: 0.2952623 (lr:0.00011785825767278981)
10200: accuracy:1.0 loss: 0.66008204 (lr:0.00011768056503999535)
10220: accuracy:0.98 loss: 3.3622959 (lr:0.00011750464047843875)
10240: accuracy:1.0 loss: 0.20779337 (lr:0.00011733046639551725)
10260: accuracy:1.0 loss: 0.52857316 (lr:0.0001171580253736774)
10280: accuracy:0.99 loss: 1.4034648 (lr:0.00011698730016867331)
10300: accuracy:1.0 loss: 0.2581988 (lr:0.00011681827370784222)
10320: accuracy:1.0 loss: 0.93277067 (lr:0.00011665092908839719)
10340: accuracy:1.0 loss: 0.48814994 (lr:0.00011648524957573683)
10360: accuracy:1.0 loss: 0.9279463 (lr:0.00011632121860177179)
10380: accuracy:1.0 loss: 0.8345069 (lr:0.00011615881976326798)
10400: accuracy:1.0 loss: 0.219197 (lr:0.00011599803682020625)
10420: accuracy:1.0 loss: 0.1861228 (lr:0.00011583885369415826)
10440: accuracy:1.0 loss: 0.7377313 (lr:0.00011568125446667879)
10460: accuracy:1.0 loss: 0.33992863 (lr:0.00011552522337771372)
10480: accuracy:1.0 loss: 0.43866915 (lr:0.00011537074482402417)
10500: accuracy:0.99 loss: 5.381299 (lr:0.00011521780335762602)
10520: accuracy:0.99 loss: 2.3651192 (lr:0.00011506638368424518)
10540: accuracy:1.0 loss: 0.41616195 (lr:0.00011491647066178812)
10560: accuracy:1.0 loss: 0.07018836 (lr:0.00011476804929882766)
10580: accuracy:1.0 loss: 0.057824165 (lr:0.00011462110475310384)
10600: accuracy:1.0 loss: 0.11775974 (lr:0.00011447562233003964)
10604: ********* epoch 12 ********* test accuracy:0.8166508893428329
10620: accuracy:1.0 loss: 0.13741796 (lr:0.00011433158748127149)
10640: accuracy:1.0 loss: 0.17702226 (lr:0.00011418898580319449)
10660: accuracy:1.0 loss: 0.30976003 (lr:0.00011404780303552202)
10680: accuracy:1.0 loss: 0.33651277 (lr:0.00011390802505985963)
10700: accuracy:0.99 loss: 10.02608 (lr:0.0001137696378982933)
10720: accuracy:1.0 loss: 0.72230726 (lr:0.0001136326277119915)
10740: accuracy:1.0 loss: 0.2488292 (lr:0.0001134969807998215)
10760: accuracy:1.0 loss: 0.5218401 (lr:0.00011336268359697898)
10780: accuracy:1.0 loss: 0.48459378 (lr:0.00011322972267363179)
10800: accuracy:1.0 loss: 0.67495364 (lr:0.00011309808473357674)
10820: accuracy:1.0 loss: 0.6544933 (lr:0.00011296775661291017)
10840: accuracy:1.0 loss: 0.72824335 (lr:0.00011283872527871139)
10860: accuracy:1.0 loss: 0.24352981 (lr:0.00011271097782773946)
10880: accuracy:1.0 loss: 0.1702466 (lr:0.0001125845014851428)
10900: accuracy:1.0 loss: 0.3200244 (lr:0.0001124592836031818)
10920: accuracy:1.0 loss: 0.22248738 (lr:0.00011233531165996387)
10940: accuracy:1.0 loss: 0.8479277 (lr:0.00011221257325819141)
10960: accuracy:1.0 loss: 0.25728145 (lr:0.00011209105612392192)
10980: accuracy:1.0 loss: 0.48019248 (lr:0.00011197074810534076)
11000: accuracy:1.0 loss: 0.35221934 (lr:0.0001118516371715458)
11020: accuracy:1.0 loss: 0.17052361 (lr:0.00011173371141134438)
11040: accuracy:1.0 loss: 0.49381372 (lr:0.00011161695903206223)
11060: accuracy:0.99 loss: 4.0255585 (lr:0.00011150136835836409)
11080: accuracy:0.99 loss: 3.9985135 (lr:0.00011138692783108631)
11100: accuracy:1.0 loss: 0.14992951 (lr:0.00011127362600608078)
11120: accuracy:1.0 loss: 0.5555495 (lr:0.00011116145155307059)
11140: accuracy:1.0 loss: 0.283201 (lr:0.00011105039325451691)
11160: accuracy:1.0 loss: 0.48349118 (lr:0.0001109404400044974)
11180: accuracy:1.0 loss: 0.5479549 (lr:0.0001108315808075954)
11200: accuracy:1.0 loss: 0.7142703 (lr:0.00011072380477780052)
11220: accuracy:0.99 loss: 1.3813137 (lr:0.00011061710113741992)
11240: accuracy:1.0 loss: 0.20269413 (lr:0.0001105114592160007)
11260: accuracy:1.0 loss: 0.22518714 (lr:0.00011040686844926266)
11280: accuracy:1.0 loss: 1.2256062 (lr:0.00011030331837804195)
11300: accuracy:1.0 loss: 0.37215388 (lr:0.00011020079864724517)
11320: accuracy:1.0 loss: 0.33595294 (lr:0.00011009929900481383)
11340: accuracy:1.0 loss: 0.48038453 (lr:0.00010999880930069907)
11360: accuracy:1.0 loss: 0.23899302 (lr:0.00010989931948584675)
11380: accuracy:1.0 loss: 0.7107636 (lr:0.00010980081961119247)
11400: accuracy:1.0 loss: 0.6177891 (lr:0.0001097032998266667)
11420: accuracy:1.0 loss: 0.24572678 (lr:0.00010960675038020969)
11440: accuracy:1.0 loss: 0.22853343 (lr:0.00010951116161679636)
11460: accuracy:1.0 loss: 0.16786477 (lr:0.0001094165239774707)
11480: accuracy:1.0 loss: 0.16917561 (lr:0.00010932282799838993)
11500: accuracy:0.99 loss: 3.1828995 (lr:0.00010923006430987804)
11520: accuracy:1.0 loss: 0.44870606 (lr:0.00010913822363548888)
11540: accuracy:1.0 loss: 0.129913 (lr:0.00010904729679107849)
11560: accuracy:1.0 loss: 0.1033184 (lr:0.00010895727468388663)
11568: ********* epoch 13 ********* test accuracy:0.8193413382492153
11580: accuracy:1.0 loss: 0.12126248 (lr:0.0001088681483116276)
11600: accuracy:1.0 loss: 0.08612831 (lr:0.00010877990876158987)
11620: accuracy:1.0 loss: 0.16598132 (lr:0.00010869254720974492)
11640: accuracy:1.0 loss: 0.13428466 (lr:0.00010860605491986474)
11660: accuracy:1.0 loss: 0.29128516 (lr:0.0001085204232426483)
11680: accuracy:1.0 loss: 0.21102118 (lr:0.0001084356436148565)
11700: accuracy:0.99 loss: 11.728931 (lr:0.00010835170755845591)
11720: accuracy:1.0 loss: 0.92331696 (lr:0.00010826860667977094)
11740: accuracy:1.0 loss: 0.30509225 (lr:0.00010818633266864447)
11760: accuracy:1.0 loss: 0.7442218 (lr:0.00010810487729760686)
11780: accuracy:1.0 loss: 0.1482573 (lr:0.00010802423242105307)
11800: accuracy:1.0 loss: 0.46725422 (lr:0.00010794438997442827)
11820: accuracy:1.0 loss: 0.20271356 (lr:0.00010786534197342127)
11840: accuracy:1.0 loss: 0.844096 (lr:0.00010778708051316608)
11860: accuracy:1.0 loss: 0.97858363 (lr:0.00010770959776745147)
11880: accuracy:1.0 loss: 0.18792504 (lr:0.00010763288598793828)
11900: accuracy:1.0 loss: 1.0308353 (lr:0.00010755693750338465)
11920: accuracy:1.0 loss: 0.3563736 (lr:0.00010748174471887883)
11940: accuracy:1.0 loss: 0.2585842 (lr:0.0001074073001150797)
11960: accuracy:1.0 loss: 0.4250585 (lr:0.00010733359624746485)
11980: accuracy:1.0 loss: 0.31263417 (lr:0.0001072606257455861)
12000: accuracy:1.0 loss: 0.35236683 (lr:0.00010718838131233245)
12020: accuracy:1.0 loss: 0.6601419 (lr:0.00010711685572320037)
12040: accuracy:1.0 loss: 0.31687957 (lr:0.00010704604182557134)
12060: accuracy:1.0 loss: 0.3271722 (lr:0.00010697593253799658)
12080: accuracy:1.0 loss: 0.36226723 (lr:0.00010690652084948893)
12100: accuracy:1.0 loss: 0.5281899 (lr:0.00010683779981882168)
12120: accuracy:1.0 loss: 0.17351156 (lr:0.0001067697625738345)
12140: accuracy:1.0 loss: 0.17101642 (lr:0.00010670240231074618)
12160: accuracy:1.0 loss: 0.40029198 (lr:0.0001066357122934743)
12180: accuracy:1.0 loss: 0.5087053 (lr:0.00010656968585296154)
12200: accuracy:1.0 loss: 0.87241393 (lr:0.00010650431638650884)
12220: accuracy:1.0 loss: 0.55183 (lr:0.00010643959735711506)
12240: accuracy:1.0 loss: 0.76618713 (lr:0.00010637552229282334)
12260: accuracy:1.0 loss: 0.09086697 (lr:0.00010631208478607387)
12280: accuracy:1.0 loss: 0.2917984 (lr:0.00010624927849306309)
12300: accuracy:1.0 loss: 0.7935876 (lr:0.00010618709713310935)
12320: accuracy:1.0 loss: 0.34013367 (lr:0.00010612553448802488)
12340: accuracy:1.0 loss: 0.2948194 (lr:0.00010606458440149382)
12360: accuracy:1.0 loss: 0.50206983 (lr:0.00010600424077845677)
12380: accuracy:1.0 loss: 0.35043883 (lr:0.00010594449758450108)
12400: accuracy:1.0 loss: 0.5447852 (lr:0.00010588534884525763)
12420: accuracy:1.0 loss: 0.43163648 (lr:0.00010582678864580318)
12440: accuracy:0.99 loss: 1.3444433 (lr:0.00010576881113006898)
12460: accuracy:1.0 loss: 0.41975397 (lr:0.00010571141050025514)
12480: accuracy:0.99 loss: 1.3400698 (lr:0.00010565458101625086)
12500: accuracy:1.0 loss: 0.039531346 (lr:0.00010559831699506036)
12520: accuracy:1.0 loss: 0.07611175 (lr:0.00010554261281023466)
12532: ********* epoch 14 ********* test accuracy:0.8248218823177719
12540: accuracy:1.0 loss: 0.0280263 (lr:0.00010548746289130884)
12560: accuracy:1.0 loss: 0.075284034 (lr:0.00010543286172324504)
12580: accuracy:1.0 loss: 0.23654687 (lr:0.00010537880384588097)
12600: accuracy:1.0 loss: 0.17713168 (lr:0.00010532528385338383)
12620: accuracy:1.0 loss: 0.53516734 (lr:0.00010527229639370979)
12640: accuracy:0.99 loss: 2.9076881 (lr:0.00010521983616806871)
12660: accuracy:1.0 loss: 0.76861596 (lr:0.00010516789793039434)
12680: accuracy:1.0 loss: 0.13386177 (lr:0.00010511647648681958)
12700: accuracy:1.0 loss: 0.22855574 (lr:0.00010506556669515726)
12720: accuracy:1.0 loss: 0.2274398 (lr:0.00010501516346438575)
12740: accuracy:1.0 loss: 0.399118 (lr:0.00010496526175414002)
12760: accuracy:1.0 loss: 0.13940267 (lr:0.00010491585657420743)
12780: accuracy:1.0 loss: 0.82749975 (lr:0.00010486694298402883)
12800: accuracy:1.0 loss: 0.44699568 (lr:0.00010481851609220441)
12820: accuracy:1.0 loss: 0.19700676 (lr:0.00010477057105600468)
12840: accuracy:1.0 loss: 0.71004903 (lr:0.00010472310308088602)
12860: accuracy:0.99 loss: 1.288969 (lr:0.00010467610742001139)
12880: accuracy:1.0 loss: 0.21724817 (lr:0.00010462957937377552)
12900: accuracy:1.0 loss: 0.16473281 (lr:0.00010458351428933503)
12920: accuracy:1.0 loss: 0.2243973 (lr:0.00010453790756014309)
12940: accuracy:1.0 loss: 0.15641923 (lr:0.00010449275462548875)
12960: accuracy:1.0 loss: 0.7709157 (lr:0.00010444805097004095)
12980: accuracy:1.0 loss: 0.13706033 (lr:0.00010440379212339686)
13000: accuracy:1.0 loss: 0.35853198 (lr:0.00010435997365963497)
13020: accuracy:1.0 loss: 0.6508721 (lr:0.00010431659119687235)
13040: accuracy:1.0 loss: 0.18311833 (lr:0.00010427364039682659)
13060: accuracy:1.0 loss: 0.2391739 (lr:0.00010423111696438189)
13080: accuracy:1.0 loss: 0.27304164 (lr:0.00010418901664715958)
13100: accuracy:1.0 loss: 0.33116892 (lr:0.00010414733523509284)
13120: accuracy:1.0 loss: 0.13683078 (lr:0.00010410606856000573)
13140: accuracy:1.0 loss: 0.4189055 (lr:0.00010406521249519637)
13160: accuracy:1.0 loss: 0.33647636 (lr:0.0001040247629550242)
13180: accuracy:1.0 loss: 0.1634051 (lr:0.00010398471589450152)
13200: accuracy:1.0 loss: 0.13466604 (lr:0.0001039450673088889)
13220: accuracy:1.0 loss: 0.26176998 (lr:0.00010390581323329473)
13240: accuracy:1.0 loss: 0.21699029 (lr:0.00010386694974227874)
13260: accuracy:1.0 loss: 0.18683094 (lr:0.00010382847294945946)
13280: accuracy:1.0 loss: 0.21201365 (lr:0.00010379037900712552)
13300: accuracy:1.0 loss: 0.12693651 (lr:0.00010375266410585096)
13320: accuracy:1.0 loss: 0.14181754 (lr:0.00010371532447411422)
13340: accuracy:1.0 loss: 1.0622185 (lr:0.000103678356377921)
13360: accuracy:1.0 loss: 0.27410397 (lr:0.00010364175612043088)
13380: accuracy:1.0 loss: 0.05626555 (lr:0.0001036055200415876)
13400: accuracy:1.0 loss: 0.18415245 (lr:0.0001035696445177531)
13420: accuracy:1.0 loss: 0.59549785 (lr:0.00010353412596134509)
13440: accuracy:0.99 loss: 3.2686954 (lr:0.00010349896082047832)
13460: accuracy:1.0 loss: 0.40527865 (lr:0.00010346414557860941)
13480: accuracy:1.0 loss: 0.028203292 (lr:0.00010342967675418516)
13496: ********* epoch 15 ********* test accuracy:0.8309501270489761
13500: accuracy:1.0 loss: 0.05537754 (lr:0.00010339555090029441)
13520: accuracy:1.0 loss: 0.083975 (lr:0.00010336176460432333)
13540: accuracy:1.0 loss: 0.53562117 (lr:0.00010332831448761414)
13560: accuracy:1.0 loss: 0.2835279 (lr:0.00010329519720512733)
13580: accuracy:1.0 loss: 0.07549904 (lr:0.00010326240944510704)
13600: accuracy:1.0 loss: 0.35895148 (lr:0.00010322994792874994)
13620: accuracy:1.0 loss: 0.12394454 (lr:0.00010319780940987735)
13640: accuracy:1.0 loss: 0.21691054 (lr:0.00010316599067461059)
13660: accuracy:1.0 loss: 0.06951835 (lr:0.00010313448854104963)
13680: accuracy:1.0 loss: 0.07665469 (lr:0.00010310329985895485)
13700: accuracy:1.0 loss: 0.077169746 (lr:0.00010307242150943207)
13720: accuracy:1.0 loss: 0.19788685 (lr:0.00010304185040462059)
13740: accuracy:1.0 loss: 0.23022695 (lr:0.00010301158348738446)
13760: accuracy:1.0 loss: 0.18430085 (lr:0.00010298161773100672)
13780: accuracy:1.0 loss: 0.21732755 (lr:0.00010295195013888678)
13800: accuracy:1.0 loss: 0.4198989 (lr:0.00010292257774424069)
13820: accuracy:0.99 loss: 2.9302814 (lr:0.00010289349760980451)
13840: accuracy:1.0 loss: 0.12140225 (lr:0.00010286470682754057)
13860: accuracy:1.0 loss: 0.13082924 (lr:0.00010283620251834665)
13880: accuracy:1.0 loss: 0.16491193 (lr:0.00010280798183176808)
13900: accuracy:1.0 loss: 0.23469014 (lr:0.00010278004194571267)
13920: accuracy:1.0 loss: 0.37990287 (lr:0.00010275238006616854)
13940: accuracy:1.0 loss: 0.17629051 (lr:0.00010272499342692468)
13960: accuracy:1.0 loss: 0.15121336 (lr:0.00010269787928929435)
13980: accuracy:1.0 loss: 0.066247046 (lr:0.0001026710349418412)
14000: accuracy:1.0 loss: 0.13380392 (lr:0.00010264445770010811)
14020: accuracy:1.0 loss: 0.16787064 (lr:0.00010261814490634873)
14040: accuracy:0.99 loss: 3.043124 (lr:0.0001025920939292618)
14060: accuracy:1.0 loss: 0.111209564 (lr:0.00010256630216372787)
14080: accuracy:1.0 loss: 0.10473072 (lr:0.0001025407670305489)
14100: accuracy:1.0 loss: 0.1410195 (lr:0.00010251548597619031)
14120: accuracy:1.0 loss: 0.3870002 (lr:0.00010249045647252558)
14140: accuracy:1.0 loss: 0.701615 (lr:0.0001024656760165835)
14160: accuracy:1.0 loss: 0.26029947 (lr:0.0001024411421302978)
14180: accuracy:1.0 loss: 0.14428484 (lr:0.00010241685236025943)
14200: accuracy:1.0 loss: 0.2718048 (lr:0.00010239280427747112)
14220: accuracy:1.0 loss: 0.17284536 (lr:0.00010236899547710459)
14240: accuracy:1.0 loss: 0.16852318 (lr:0.00010234542357825993)
14260: accuracy:1.0 loss: 0.17250575 (lr:0.00010232208622372762)
14280: accuracy:0.99 loss: 1.5641806 (lr:0.00010229898107975277)
14300: accuracy:0.99 loss: 1.8619514 (lr:0.0001022761058358017)
14320: accuracy:1.0 loss: 0.08405293 (lr:0.00010225345820433099)
14340: accuracy:1.0 loss: 0.078173876 (lr:0.00010223103592055859)
14360: accuracy:1.0 loss: 0.36923438 (lr:0.00010220883674223746)
14380: accuracy:1.0 loss: 0.10476876 (lr:0.00010218685844943125)
14400: accuracy:1.0 loss: 0.13026592 (lr:0.00010216509884429238)
14420: accuracy:1.0 loss: 0.0397638 (lr:0.00010214355575084218)
14440: accuracy:1.0 loss: 0.04453382 (lr:0.00010212222701475337)
14460: ********* epoch 16 ********* test accuracy:0.8221812565392855
14460: accuracy:1.0 loss: 0.016324159 (lr:0.00010210111050313457)
14480: accuracy:1.0 loss: 0.02341574 (lr:0.00010208020410431701)
14500: accuracy:1.0 loss: 0.10496816 (lr:0.0001020595057276434)
14520: accuracy:1.0 loss: 0.0571437 (lr:0.00010203901330325881)
14540: accuracy:1.0 loss: 0.05020681 (lr:0.00010201872478190372)
14560: accuracy:1.0 loss: 0.16693541 (lr:0.0001019986381347091)
14580: accuracy:1.0 loss: 0.31505945 (lr:0.0001019787513529935)
14600: accuracy:1.0 loss: 0.64964825 (lr:0.00010195906244806215)
14620: accuracy:1.0 loss: 0.15469572 (lr:0.00010193956945100817)
14640: accuracy:1.0 loss: 0.022209035 (lr:0.00010192027041251561)
14660: accuracy:1.0 loss: 0.09125778 (lr:0.00010190116340266453)
14680: accuracy:1.0 loss: 0.3988685 (lr:0.00010188224651073801)
14700: accuracy:1.0 loss: 0.08664432 (lr:0.00010186351784503112)
14720: accuracy:1.0 loss: 0.05011148 (lr:0.00010184497553266167)
14740: accuracy:1.0 loss: 0.15682982 (lr:0.00010182661771938297)
14760: accuracy:1.0 loss: 0.14591525 (lr:0.00010180844256939839)
14780: accuracy:1.0 loss: 0.5976164 (lr:0.0001017904482651778)
14800: accuracy:1.0 loss: 0.12637344 (lr:0.00010177263300727576)
14820: accuracy:1.0 loss: 0.16132595 (lr:0.00010175499501415165)
14840: accuracy:1.0 loss: 0.22232823 (lr:0.00010173753252199147)
14860: accuracy:1.0 loss: 0.1616901 (lr:0.00010172024378453143)
14880: accuracy:1.0 loss: 0.10861724 (lr:0.00010170312707288338)
14900: accuracy:1.0 loss: 0.18075153 (lr:0.0001016861806753619)
14920: accuracy:1.0 loss: 0.5796125 (lr:0.0001016694028973131)
14940: accuracy:1.0 loss: 0.016743707 (lr:0.00010165279206094521)
14960: accuracy:1.0 loss: 0.13698524 (lr:0.00010163634650516076)
14980: accuracy:1.0 loss: 0.1079097 (lr:0.00010162006458539044)
15000: accuracy:1.0 loss: 0.124785945 (lr:0.00010160394467342873)
15020: accuracy:1.0 loss: 0.16896208 (lr:0.00010158798515727098)
15040: accuracy:1.0 loss: 1.0394913 (lr:0.00010157218444095227)
15060: accuracy:1.0 loss: 0.075945035 (lr:0.00010155654094438782)
15080: accuracy:1.0 loss: 0.10167942 (lr:0.00010154105310321494)
15100: accuracy:1.0 loss: 0.19486567 (lr:0.00010152571936863659)
15120: accuracy:1.0 loss: 0.07423391 (lr:0.00010151053820726654)
15140: accuracy:1.0 loss: 0.35585263 (lr:0.00010149550810097601)
15160: accuracy:1.0 loss: 0.16618152 (lr:0.00010148062754674182)
15180: accuracy:1.0 loss: 0.21821988 (lr:0.00010146589505649618)
15200: accuracy:1.0 loss: 0.4324592 (lr:0.00010145130915697777)
15220: accuracy:1.0 loss: 0.06794419 (lr:0.0001014368683895845)
15240: accuracy:1.0 loss: 0.07550351 (lr:0.00010142257131022757)
15260: accuracy:1.0 loss: 0.17338824 (lr:0.00010140841648918713)
15280: accuracy:1.0 loss: 0.04493695 (lr:0.00010139440251096931)
15300: accuracy:1.0 loss: 0.060643755 (lr:0.00010138052797416459)
15320: accuracy:0.99 loss: 3.954783 (lr:0.00010136679149130772)
15340: accuracy:1.0 loss: 0.65078205 (lr:0.000101353191688739)
15360: accuracy:1.0 loss: 0.15792958 (lr:0.0001013397272064668)
15380: accuracy:1.0 loss: 0.022780716 (lr:0.00010132639669803168)
15400: accuracy:1.0 loss: 0.011010761 (lr:0.00010131319883037172)
15420: accuracy:1.0 loss: 0.024599472 (lr:0.00010130013228368913)
15424: ********* epoch 17 ********* test accuracy:0.8201634198594988
